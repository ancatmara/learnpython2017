{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim, word2vec\n",
    "\n",
    "Gensim - вообще библиотека для тематического моделирования текстов. Один из компонентов в ней - реализация на python алгоритмов из библиотеки word2vec (которая в оригинале была написана на C++).\n",
    "\n",
    "Если gensim у вас не стоит, то ставим:\n",
    "\n",
    "`pip install gensim`\n",
    "\n",
    "Поскольку иногда тренировка модели занимает много времени, то можно ещё вести лог событий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Процесс тренировки модели \n",
    "\n",
    "На вход модели даем текстовый файл, каждое предложение на отдельной строчке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = 'text.txt'\n",
    "data = gensim.models.word2vec.LineSentence(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем модель. Параметры в скобочках:\n",
    "* data - данные, \n",
    "* size - размер вектора, \n",
    "* window - размер окна наблюдения,\n",
    "* min_count - мин. частотность слова в корпусе, которое мы берем,\n",
    "* sg - используемый алгоритм обучение (0 - CBOW, 1 - Skip-gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-30 00:13:36,174 : INFO : collecting all words and their counts\n",
      "2017-05-30 00:13:36,178 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-05-30 00:13:36,304 : INFO : collected 30172 word types from a corpus of 68570 raw words and 5432 sentences\n",
      "2017-05-30 00:13:36,307 : INFO : Loading a fresh vocabulary\n",
      "2017-05-30 00:13:36,408 : INFO : min_count=2 retains 6793 unique words (22% of original 30172, drops 23379)\n",
      "2017-05-30 00:13:36,409 : INFO : min_count=2 leaves 45191 word corpus (65% of original 68570, drops 23379)\n",
      "2017-05-30 00:13:36,467 : INFO : deleting the raw counts dictionary of 30172 items\n",
      "2017-05-30 00:13:36,471 : INFO : sample=0.001 downsamples 34 most-common words\n",
      "2017-05-30 00:13:36,474 : INFO : downsampling leaves estimated 37321 word corpus (82.6% of prior 45191)\n",
      "2017-05-30 00:13:36,478 : INFO : estimated required memory for 6793 words and 500 dimensions: 30568500 bytes\n",
      "2017-05-30 00:13:36,535 : INFO : resetting layer weights\n",
      "2017-05-30 00:13:36,897 : INFO : training model with 3 workers on 6793 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-05-30 00:13:37,933 : INFO : PROGRESS: at 74.04% examples, 137499 words/s, in_qsize 6, out_qsize 0\n",
      "2017-05-30 00:13:38,133 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-05-30 00:13:38,147 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-05-30 00:13:38,164 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-05-30 00:13:38,166 : INFO : training on 342850 raw words (186531 effective words) took 1.3s, 147698 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(data, size=500, window=10, min_count=2, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно нормализовать вектора, тогда модель будет занимать меньше RAM. Однако после этого её нельзя дотренировывать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-30 00:13:40,834 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим, сколько в модели слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6793\n"
     ]
    }
   ],
   "source": [
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сохраняем!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-30 00:15:07,615 : INFO : saving Word2Vec object under my.model, separately None\n",
      "2017-05-30 00:15:07,621 : INFO : not storing attribute syn0norm\n",
      "2017-05-30 00:15:07,623 : INFO : not storing attribute cum_table\n",
      "2017-05-30 00:15:08,144 : INFO : saved my.model\n"
     ]
    }
   ],
   "source": [
    "model.save('my.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с моделью\n",
    "\n",
    "Для каких-то своих индивидуальных нужд и экспериментов бывает полезно самому натренировать модель на нужных данных и с нужными параметрами. Но для каких-то общих целей модели уже есть как для русского языка, так и для английского.\n",
    "\n",
    "Модели для русского скачать можно здесь - http://rusvectores.org/ru/models\n",
    "\n",
    "Скачаем модель для русского языка, созданную на основе НКРЯ. Поскольку модели бывают разных форматов, то для них написаны разные функции загрузки; бывает полезно учитывать это в своем скрипте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-30 00:15:15,307 : INFO : loading projection weights from ruscorpora_1_300_10.bin.gz\n",
      "2017-05-30 00:15:35,994 : INFO : loaded (184973, 300) matrix from ruscorpora_1_300_10.bin.gz\n"
     ]
    }
   ],
   "source": [
    "m = 'ruscorpora_1_300_10.bin.gz'\n",
    "if m.endswith('.vec.gz'):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=False)\n",
    "elif m.endswith('.bin.gz'):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=True)\n",
    "else:\n",
    "    model = gensim.models.KeyedVectors.load(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-05-30 00:15:40,106 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скажем, нам интересны такие слова (пример для русского языка):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = ['день_NOUN', 'ночь_NOUN', 'человек_NOUN', 'семантика_NOUN', 'студент_NOUN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частеречные тэги нужны, поскольку это специфика скачанной модели - она была натренирована на словах, аннотированных их частями речи (и лемматизированных).\n",
    "\n",
    "Попросим у модели 10 ближайших соседей для каждого слова и коэффициент косинусной близости для каждого:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "день_NOUN\n",
      "[ 0.02765099 -0.06994063  0.02292976  0.00478499 -0.0373687   0.06778284\n",
      "  0.01259792 -0.09310838  0.10806882 -0.01369707]\n",
      "неделя_NOUN 0.7186020016670227\n",
      "утро_NOUN 0.6839577555656433\n",
      "месяц_NOUN 0.6676345467567444\n",
      "вечер_NOUN 0.6635110378265381\n",
      "час_NOUN 0.619712233543396\n",
      "полдень_NOUN 0.6096612215042114\n",
      "суббота_NOUN 0.6011918783187866\n",
      "ночь_NOUN 0.5900086164474487\n",
      "накануне_ADV 0.5895365476608276\n",
      "понедельник_NOUN 0.5813597440719604\n",
      "\n",
      "\n",
      "ночь_NOUN\n",
      "[ 0.00090496 -0.09171917  0.06209332  0.00910732 -0.03711092  0.0487812\n",
      "  0.05916326 -0.12525907  0.02495096 -0.07734441]\n",
      "вечер_NOUN 0.7375167608261108\n",
      "рассвет_NOUN 0.7365224361419678\n",
      "утро_NOUN 0.6998549103736877\n",
      "полночь_NOUN 0.6829675436019897\n",
      "напролет_ADV 0.6194927096366882\n",
      "сумерки_NOUN 0.6130234003067017\n",
      "ночной_ADJ 0.6043827533721924\n",
      "бессонный_ADJ 0.5970311760902405\n",
      "спать_VERB 0.5932828187942505\n",
      "полдень_NOUN 0.5922213196754456\n",
      "\n",
      "\n",
      "человек_NOUN\n",
      "[-0.13945162  0.00953103  0.09669054 -0.0521334  -0.03127262  0.05055281\n",
      " -0.02525845  0.01031411  0.0670734  -0.00500829]\n",
      "женщина_NOUN 0.5500056743621826\n",
      "мужчина_NOUN 0.5161216855049133\n",
      "человеческий_ADJ 0.5005477666854858\n",
      "идолопоклонствовать_VERB 0.4838884770870209\n",
      "высокопорядочный_ADJ 0.4818764925003052\n",
      "правдознатец_NOUN 0.48151782155036926\n",
      "некорыстолюбивый_ADJ 0.4798555374145508\n",
      "народ_NOUN 0.477242112159729\n",
      "старое::стариться_VERB 0.4748774468898773\n",
      "людишки_NOUN 0.4739970862865448\n",
      "\n",
      "\n",
      "семантика_NOUN\n",
      "[ 0.04162881  0.03207609 -0.00509921  0.10019507 -0.03660124 -0.03263019\n",
      " -0.01842243 -0.02713792  0.02342849 -0.09310298]\n",
      "семантический_ADJ 0.7749344110488892\n",
      "синтаксический_ADJ 0.6857521533966064\n",
      "лексический::семантика_NOUN 0.6853606104850769\n",
      "грамматикализация_NOUN 0.6655354499816895\n",
      "семантически_ADV 0.6494707465171814\n",
      "аспектуальный_ADJ 0.63737553358078\n",
      "семантический::метаязык_NOUN 0.6367851495742798\n",
      "синтаксический::актант_NOUN 0.6321083307266235\n",
      "wierzbicka_NOUN 0.6281548738479614\n",
      "сочетаемость_NOUN 0.6265926361083984\n",
      "\n",
      "\n",
      "студент_NOUN\n",
      "[-0.02158827  0.0493357  -0.0394694   0.00178204  0.02384631 -0.05032327\n",
      "  0.03192617 -0.05180086  0.00875538  0.05471088]\n",
      "преподаватель_NOUN 0.6941744089126587\n",
      "университет_NOUN 0.683201014995575\n",
      "студентка_NOUN 0.6738083362579346\n",
      "аспирант_NOUN 0.6682658195495605\n",
      "студенческий_ADJ 0.665415346622467\n",
      "факультет_NOUN 0.6491575241088867\n",
      "первокурсник_NOUN 0.6433141231536865\n",
      "универсант_NOUN 0.6419535875320435\n",
      "профессор_NOUN 0.6393071413040161\n",
      "ректор_NOUN 0.6145827174186707\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    # есть ли слово в модели? Может быть, и нет\n",
    "    if word in model:\n",
    "        print(word)\n",
    "        # смотрим на вектор слова (его размерность 300, смотрим на первые 10 чисел)\n",
    "        print(model[word][:10])\n",
    "        # выдаем 10 ближайших соседей слова:\n",
    "        for i in model.most_similar(positive=[word], topn=10):\n",
    "            # слово + коэффициент косинусной близости\n",
    "            print(i[0], i[1])\n",
    "        print('\\n')\n",
    "    else:\n",
    "        # Увы!\n",
    "        print(word + ' is not present in the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим косинусную близость пары слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.330222839481\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('человек_NOUN', 'обезьяна_NOUN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найди лишнее!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "картофель_NOUN\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match('яблоко_NOUN груша_NOUN виноград_NOUN банан_NOUN лимон_NOUN картофель_NOUN'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реши пропорцию!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пельмень_NOUN\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['пицца_NOUN', 'россия_NOUN'], negative=['италия_NOUN'])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это конечно все хорошо, но как понять, какая модель лучше? Или вот например я сделал свою модель, насколько она хорошая?\n",
    "\n",
    "Для этого существуют специальные датасеты для оценки качества дистрибутивных моделей. Их два: один измеряет точность решения задач на аналогии (про Россию и пельмени), а второй используется для оценки коэффициента семантической близости. Подробнее читаем тут:  \n",
    "* http://www.aclweb.org/aclwiki/index.php?title=Google_analogy_test_set_(State_of_the_art)\n",
    "* https://www.aclweb.org/aclwiki/index.php?title=SimLex-999_(State_of_the_art)\n",
    "\n",
    "Датасеты для русского языка можно скачать на странице с моделями на RusVectores. Посчитаем качество нашей модели НКРЯ на датасете про аналогии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.accuracy('ruanalogy_upos.txt', restrict_vocab=3000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}